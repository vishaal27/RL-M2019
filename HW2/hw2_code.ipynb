{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: GridWorld with solving Bellman equations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.3,  8.8,  4.4,  5.3,  1.5],\n",
       "       [ 1.5,  3. ,  2.3,  1.9,  0.5],\n",
       "       [ 0.1,  0.7,  0.7,  0.4, -0.4],\n",
       "       [-1. , -0.4, -0.4, -0.6, -1.2],\n",
       "       [-1.9, -1.3, -1.2, -1.4, -2. ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# discount rate gamma\n",
    "discount_rate=0.9\n",
    "\n",
    "# constant variables regarding state environments\n",
    "num_rows=5\n",
    "\n",
    "num_cols=5\n",
    "num_states=25\n",
    "\n",
    "# coefficient matrix and constant vector for solving system of linear equations\n",
    "grid_world_state_coefficients=np.zeros((num_states, num_states))\n",
    "grid_world_constants=np.zeros(num_states)\n",
    "\n",
    "# possible action sequences in any given state\n",
    "actions={'north':(-1, 0), 'south':(1, 0), 'east':(0, 1), 'west':(0, -1)}\n",
    "\n",
    "# policy for each action action taken (in this case equiprobable)\n",
    "pi=0.25\n",
    "\n",
    "# Given a current state and an action taken, returns the next state and the reward obtained\n",
    "def next_state_and_reward(current_state, current_action):\n",
    "    reward=0\n",
    "    next_state=[0, 0]\n",
    "        \n",
    "    # if current state is state A, then reward is 10 and next state is A' regardless of the action taken\n",
    "    if(current_state[0]==0 and current_state[1]==1):\n",
    "        reward=10\n",
    "        next_state=[4, 1]\n",
    "        return reward, next_state\n",
    "    \n",
    "    # if current state is state B, then reward is 5 and next state is B' regardless of the action taken\n",
    "    elif(current_state[0]==0 and current_state[1]==3):\n",
    "        reward=5\n",
    "        next_state=[2, 3]\n",
    "        return reward, next_state\n",
    "    \n",
    "    # if next state that is reached based on action taken is within the grid world, then return next state with reward 0\n",
    "    if((current_state[0]+current_action[0])>=0 and (current_state[0]+current_action[0])<=4 and (current_state[1]+current_action[1])>=0 and (current_state[1]+current_action[1])<=4):\n",
    "        reward=0\n",
    "        next_state=[current_state[0]+current_action[0], current_state[1]+current_action[1]]        \n",
    "        return reward, next_state\n",
    "    \n",
    "    # if next state that is reached based on action taken is outside gridworld, then return next state as current state with reward -1\n",
    "    else:\n",
    "        reward=-1\n",
    "        next_state=[current_state[0], current_state[1]]\n",
    "        return reward, next_state\n",
    "\n",
    "# create coefficient and constant matrices for solving linear system of equations\n",
    "for state in range(num_states):\n",
    "    # The coefficient for a state s in its own state equation (row s) will be: \n",
    "    # c=1-n_r*(pi*prob(s,r|s,a)*gamma)\n",
    "    #\n",
    "    # The coefficient for a different state s' in a different state's state equation (row s) will be:\n",
    "    # c'=1-pi*prob(s',r|s,a)*gamma\n",
    "    #\n",
    "    # The constant term for an equation for state s (row s) will be:\n",
    "    # const=pi*(r1+r2+r3+r4)\n",
    "    # where r1 is the reward from action north, r2 is from action south, r3 from action east and r4 from action west\n",
    "    grid_world_state_coefficients[state][state]+=1\n",
    "    \n",
    "    for action in actions:\n",
    "        reward, next_state=next_state_and_reward([state%num_rows, state//num_rows], actions[action])\n",
    "        grid_world_state_coefficients[state, next_state[0]+next_state[1]*num_rows]-=pi*discount_rate\n",
    "        grid_world_constants[state]+=pi*reward\n",
    "\n",
    "# Solve system of linear equations to get the values of each of the state-action value functions\n",
    "value_function=np.linalg.solve(grid_world_state_coefficients, grid_world_constants)\n",
    "value_function.round(1).reshape((num_rows, num_cols)).transpose()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Optimal State-Value Function and Optimal Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value function is:\n",
      "[[22.  24.4 22.  19.4 17.5]\n",
      " [19.8 22.  19.8 17.8 16. ]\n",
      " [17.8 19.8 17.8 16.  14.4]\n",
      " [16.  17.8 16.  14.4 13. ]\n",
      " [14.4 16.  14.4 13.  11.7]]\n",
      "\n",
      "The optimal policy is:\n",
      "E,WSEN,W,WSEN,W\n",
      "EN,N,WN,W,W\n",
      "EN,N,WN,WN,WN\n",
      "EN,N,WN,WN,WN\n",
      "EN,N,WN,WN,WN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "# discount rate gamma\n",
    "discount_rate=0.9\n",
    "\n",
    "# constant variables regarding state environments\n",
    "num_rows=5\n",
    "num_cols=5\n",
    "num_states=25\n",
    "num_actions=4\n",
    "\n",
    "# coefficient matrix and constant vector for solving non linear system of equations for optimal action-value function\n",
    "optimal_action_coefficients=np.zeros((num_actions*num_states, num_states))\n",
    "optimal_action_constants=np.zeros(num_actions*num_states)\n",
    "\n",
    "# possible action sequences in any given state\n",
    "actions={'north':(-1, 0), 'south':(1, 0), 'east':(0, 1), 'west':(0, -1)}\n",
    "mapping={0:'W', 1:'S', 2:'E', 3:'N'}\n",
    "\n",
    "# policy for each action action taken (in this case equiprobable)\n",
    "pi=0.25\n",
    "\n",
    "# Given a current state and an action taken, returns the next state and the reward obtained\n",
    "def next_state_and_reward(current_state, current_action):\n",
    "    reward=0\n",
    "    next_state=[0, 0]\n",
    "        \n",
    "    # if current state is state A, then reward is 10 and next state is A' regardless of the action taken\n",
    "    if(current_state[0]==0 and current_state[1]==1):\n",
    "        reward=10\n",
    "        next_state=[4, 1]\n",
    "        return reward, next_state\n",
    "    \n",
    "    # if current state is state B, then reward is 5 and next state is B' regardless of the action taken\n",
    "    elif(current_state[0]==0 and current_state[1]==3):\n",
    "        reward=5\n",
    "        next_state=[2, 3]\n",
    "        return reward, next_state\n",
    "    \n",
    "    # if next state that is reached based on action taken is within the grid world, then return next state with reward 0\n",
    "    if((current_state[0]+current_action[0])>=0 and (current_state[0]+current_action[0])<=4 and (current_state[1]+current_action[1])>=0 and (current_state[1]+current_action[1])<=4):\n",
    "        reward=0\n",
    "        next_state=[current_state[0]+current_action[0], current_state[1]+current_action[1]]        \n",
    "        return reward, next_state\n",
    "    \n",
    "    # if next state that is reached based on action taken is outside gridworld, then return next state as current state with reward -1\n",
    "    else:\n",
    "        reward=-1\n",
    "        next_state=[current_state[0], current_state[1]]\n",
    "        return reward, next_state\n",
    "\n",
    "# create coefficient and constant matrices for solving non linear system of equations\n",
    "for state in range(num_states):\n",
    "    # we need to solve non linear equations of the form: Ax>=b since\n",
    "    # A is the coefficient matrix and b is the constant matrix and the bellman optimality equations are all of the form\n",
    "    # v_pi*(s)=max(.), therefore for each action we will have 4 such equations and hence there will be\n",
    "    # totally 25*4 = 100 equations\n",
    "    grid_world_state_coefficients[state][state]+=1\n",
    "    \n",
    "    for action_index, action in enumerate(actions):\n",
    "        reward, next_state=next_state_and_reward([state%num_rows, state//num_rows], actions[action])\n",
    "        optimal_action_coefficients[num_actions*state+action_index, state]+=1\n",
    "        optimal_action_coefficients[num_actions*state+action_index, next_state[0]+next_state[1]*num_rows]-=discount_rate\n",
    "        optimal_action_constants[num_actions*state+action_index]+=reward\n",
    "\n",
    "        \n",
    "# Solve system of non linear equations to get the values of each of the state-action value functions\n",
    "optimal_value_function=optimize.linprog(np.ones(num_states), -optimal_action_coefficients, -optimal_action_constants)\n",
    "optimal_value_function=np.asarray(optimal_value_function.x).round(1)\n",
    "\n",
    "print('The optimal value function is:')\n",
    "print(np.asarray(optimal_value_function.reshape((num_rows, num_cols)).transpose()))\n",
    "\n",
    "print()\n",
    "print('The optimal policy is:')\n",
    "\n",
    "# To find optimal policy from the optimal value function\n",
    "for state in range(num_states):\n",
    "    \n",
    "    # For each state, find next state from all actions\n",
    "    q_pi_values=np.zeros(num_actions)\n",
    "    for action_index, action in enumerate(actions):\n",
    "        reward, next_state=next_state_and_reward([state//num_rows, state%num_rows], actions[action])\n",
    "        q_pi_values[action_index]=optimal_value_function[next_state[0]+next_state[1]*num_rows]\n",
    "    \n",
    "    max_action=np.max(q_pi_values)\n",
    "    \n",
    "    for q_pi_value_index, q_pi_value in enumerate(q_pi_values):\n",
    "        if(q_pi_value==max_action):\n",
    "            print(mapping[q_pi_value_index], end='')\n",
    "    if((state+1)%num_rows==0):\n",
    "        print()\n",
    "    else:\n",
    "        print(',', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta:  1.8984375\n",
      "delta:  1.724609375\n",
      "delta:  1.472412109375\n",
      "delta:  1.4061737060546875\n",
      "delta:  1.3317079544067383\n",
      "delta:  1.2421786189079285\n",
      "delta:  1.1491830237209797\n",
      "delta:  1.0584387693088502\n",
      "delta:  0.9725181825488107\n",
      "delta:  0.8924059502996897\n",
      "delta:  0.8183112404493045\n",
      "delta:  0.7500779695562692\n",
      "delta:  0.6873890613115314\n",
      "delta:  0.6298669391432661\n",
      "delta:  0.5771221272796652\n",
      "delta:  0.5287760186094594\n",
      "delta:  0.48447085001008716\n",
      "delta:  0.4438733987055308\n",
      "delta:  0.4066756472311397\n",
      "delta:  0.372594031842965\n",
      "delta:  0.3413680735003908\n",
      "delta:  0.3127587833753509\n",
      "delta:  0.2865470318891994\n",
      "delta:  0.26253196938627354\n",
      "delta:  0.24052953664261167\n",
      "delta:  0.2203710789440123\n",
      "delta:  0.2019020656849193\n",
      "delta:  0.18498091196034494\n",
      "delta:  0.16947789626083676\n",
      "delta:  0.1552741675450946\n",
      "delta:  0.1422608348641461\n",
      "delta:  0.13033813295070473\n",
      "delta:  0.11941465757337255\n",
      "delta:  0.10940666489176465\n",
      "delta:  0.10023742949022107\n",
      "delta:  0.0918366561932622\n",
      "delta:  0.08413994116554946\n",
      "delta:  0.07708827817126718\n",
      "delta:  0.07062760621066388\n",
      "delta:  0.06470839506731352\n",
      "delta:  0.05928526558949798\n",
      "delta:  0.05431664179498341\n",
      "delta:  0.049764432132345604\n",
      "delta:  0.04559373745524553\n",
      "delta:  0.04177258347101542\n",
      "delta:  0.03827167561236067\n",
      "delta:  0.035064174452937635\n",
      "delta:  0.03212548994506648\n",
      "delta:  0.029433092902149127\n",
      "delta:  0.02696634228047401\n",
      "delta:  0.024706326936318135\n",
      "delta:  0.022635720645226343\n",
      "delta:  0.020738649271876852\n",
      "delta:  0.019000569072346707\n",
      "delta:  0.017408155195649755\n",
      "delta:  0.015949199529863733\n",
      "delta:  0.014612517109625855\n",
      "delta:  0.013387860367490845\n",
      "delta:  0.012265840571807018\n",
      "delta:  0.011237855848747813\n",
      "delta:  0.010296025236741002\n",
      "delta:  0.009433128267737345\n",
      "delta:  0.008642549612069672\n",
      "delta:  0.00791822836253786\n",
      "delta:  0.007254611568992431\n",
      "delta:  0.006646611667065372\n",
      "delta:  0.006089567474791835\n",
      "delta:  0.005579208458016183\n",
      "delta:  0.005111621990700144\n",
      "delta:  0.004683223359091215\n",
      "delta:  0.004290728279798373\n",
      "delta:  0.0039311277211062645\n",
      "delta:  0.003601664834476992\n",
      "delta:  0.003299813819396036\n",
      "delta:  0.003023260559515961\n",
      "delta:  0.002769884881686835\n",
      "delta:  0.0025377443018115287\n",
      "delta:  0.002325059132942897\n",
      "delta:  0.002130198841477693\n",
      "delta:  0.0019516695467842737\n",
      "delta:  0.0017881025684900465\n",
      "delta:  0.0016382439336162236\n",
      "delta:  0.0015009447630838224\n",
      "delta:  0.0013751524639253887\n",
      "delta:  0.0012599026596831209\n",
      "delta:  0.0011543117970660433\n",
      "delta:  0.0010575703722821572\n",
      "Policy evaluation update:\n",
      "[  0.   -13.99 -19.99 -21.99 -13.99 -17.99 -19.99 -19.99 -19.99 -19.99\n",
      " -17.99 -13.99 -21.99 -19.99 -13.99   0.  ]\n",
      "Policy improvement update:\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "delta:  18.989407652664127\n",
      "delta:  12.994381077746347\n",
      "Policy evaluation update:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "Policy improvement update:\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "Policy evaluation update:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "Policy improvement update:\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final optimal policy value function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "Final optimal policy\n",
      "-,W,W,WS,\n",
      "N,WN,WSEN,S,\n",
      "N,WSEN,SE,S,\n",
      "EN,E,E,-"
     ]
    }
   ],
   "source": [
    "# Policy Iteration\n",
    "\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "# parameters for MDP\n",
    "num_rows=4\n",
    "num_cols=4\n",
    "num_states=16\n",
    "grid_world=np.zeros(num_states)\n",
    "num_actions=4\n",
    "\n",
    "# mappings for optimal actions from value function\n",
    "actions={'north': [-1, 0], 'south': [1, 0], 'east': [0, 1], 'west': [0, -1]}\n",
    "mapping={0:'W', 1:'S', 2:'E', 3:'N'}\n",
    "\n",
    "# theta constraint\n",
    "theta=0.001\n",
    "\n",
    "# policy matrix containing optimal actions for each state\n",
    "pi_matrix=np.zeros((num_states, num_actions))\n",
    "pi_matrix.fill(1/num_rows)\n",
    "\n",
    "# state value function\n",
    "state_value_function=np.zeros(num_states)\n",
    "\n",
    "# return reward\n",
    "def get_reward(state, action):\n",
    "    return -1\n",
    "\n",
    "\n",
    "# get next state and reward given current state and action\n",
    "def get_next_state(state, action):\n",
    "    i, j=state//num_rows, state%num_rows\n",
    "    \n",
    "    if(i==0 and j==0):\n",
    "        next_state=[0, 0]\n",
    "        return next_state[0]*num_rows+next_state[1]\n",
    "    \n",
    "    if(i==num_rows-1 and j==num_cols-1):\n",
    "        next_state=[num_rows-1, num_cols-1]\n",
    "        return next_state[0]*num_rows+next_state[1]\n",
    "    \n",
    "    if(i+action[0]>=0 and i+action[0]<=3 and j+action[1]>=0 and j+action[1]<=3):\n",
    "        next_state=[i+action[0], j+action[1]]\n",
    "        return next_state[0]*num_rows+next_state[1]\n",
    "    \n",
    "    next_state=[i, j]\n",
    "    return next_state[0]*num_rows+next_state[1]\n",
    "  \n",
    "    \n",
    "# policy iteration loop\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Policy evaluation\n",
    "    while(True):\n",
    "        delta=0\n",
    "        for state in range(num_states):\n",
    "            v=state_value_function[state]\n",
    "            new_state_value=0\n",
    "            \n",
    "            # terminal states\n",
    "            if(state==0 or state==num_states-1):\n",
    "                continue\n",
    "            \n",
    "            # get updated state value function\n",
    "            for action_index, action in enumerate(actions):\n",
    "                new_state_value+=pi_matrix[state][action_index]*(get_reward(state, action)+state_value_function[get_next_state(state, actions[action])])\n",
    "            \n",
    "            delta=max(delta, abs(v-new_state_value))\n",
    "            state_value_function[state]=new_state_value\n",
    "        \n",
    "        # check if updated value function difference is miniscule\n",
    "        if(delta<theta):\n",
    "            break\n",
    "        print('delta: ', delta)\n",
    "    print('Policy evaluation update:')\n",
    "    print(state_value_function.round(2))\n",
    "        \n",
    "    # Policy improvement\n",
    "    \n",
    "    policy_stable=True\n",
    "    for state in range(num_states):\n",
    "        \n",
    "        # terminal states\n",
    "        if(state==0 or state==num_states-1):\n",
    "            continue\n",
    "        \n",
    "        old_action=deepcopy(pi_matrix[state])\n",
    "        \n",
    "        max_action_value=-sys.maxsize-1\n",
    "        \n",
    "        # get optimal action's value function\n",
    "        for action_index, action in enumerate(actions):\n",
    "            current_action_value=get_reward(state, action)+state_value_function[get_next_state(state, actions[action])]\n",
    "            \n",
    "            if(current_action_value>max_action_value):\n",
    "                max_action_value=current_action_value\n",
    "        \n",
    "        # get optimal actions for each state stochastically\n",
    "        best_actions=[]\n",
    "        \n",
    "        for action_index, action in enumerate(actions):\n",
    "            current_action_value=get_reward(state, action)+state_value_function[get_next_state(state, actions[action])]\n",
    "            if(current_action_value==max_action_value):\n",
    "                best_actions.append(action_index)\n",
    "        all_actions=[0, 1, 2, 3]\n",
    "        for other_action in all_actions:\n",
    "            if(other_action in best_actions):\n",
    "                pi_matrix[state][other_action]=1/len(best_actions)\n",
    "            else:\n",
    "                pi_matrix[state][other_action]=0\n",
    "        # check if updated policy is stable or not\n",
    "        for iterate in range(len(old_action)):\n",
    "            if(old_action[iterate]!=pi_matrix[state][iterate]):\n",
    "                policy_stable=False\n",
    "                break\n",
    "    \n",
    "    print('Policy improvement update:')\n",
    "    print(pi_matrix)\n",
    "    if(policy_stable):\n",
    "        break\n",
    "print()\n",
    "print('Final optimal policy value function:')\n",
    "print(state_value_function.reshape((num_rows, num_cols)))\n",
    "print()\n",
    "print('Final optimal policy')\n",
    "\n",
    "# show optimal policy\n",
    "\n",
    "for s_index, state_policy in enumerate(pi_matrix):\n",
    "    if(s_index==0):\n",
    "        print('-,', end='')\n",
    "        continue\n",
    "    if(s_index==num_states-1):\n",
    "        print('-', end='')\n",
    "        continue\n",
    "        \n",
    "    max_s=max(state_policy)\n",
    "    \n",
    "    for val in range(len(state_policy)):\n",
    "        if(state_policy[val]==max_s):\n",
    "            print(mapping[val], end='')\n",
    "            \n",
    "    print(',', end='')\n",
    "\n",
    "    if(s_index!=0 and (s_index+1)%num_rows==0):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta:  1.0\n",
      "Policy evaluation update:\n",
      "[ 0. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0.]\n",
      "delta:  1.0\n",
      "Policy evaluation update:\n",
      "[ 0. -1. -2. -2. -1. -2. -2. -2. -2. -2. -2. -1. -2. -2. -1.  0.]\n",
      "delta:  1.0\n",
      "Policy evaluation update:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "Policy improvement update:\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final optimal policy value function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "Final optimal policy\n",
      "-,W,W,WS,\n",
      "N,WN,WSEN,S,\n",
      "N,WSEN,SE,S,\n",
      "EN,E,E,-"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "# initialise parameters for MDP\n",
    "\n",
    "num_rows=4\n",
    "num_cols=4\n",
    "num_states=16\n",
    "grid_world=np.zeros(num_states)\n",
    "num_actions=4\n",
    "\n",
    "# mappings for finding optimal actions from state value function \n",
    "\n",
    "actions={'north': [-1, 0], 'south': [1, 0], 'east': [0, 1], 'west': [0, -1]}\n",
    "mapping={0:'W', 1:'S', 2:'E', 3:'N'}\n",
    "\n",
    "theta=0.001\n",
    "\n",
    "# policy matrix containing optimal actions for each state\n",
    "\n",
    "pi_matrix=np.zeros((num_states, num_actions))\n",
    "pi_matrix.fill(1/num_rows)\n",
    "\n",
    "# state value function for each state\n",
    "\n",
    "state_value_function=np.zeros(num_states)\n",
    "\n",
    "# return reward\n",
    "def get_reward(state, action):\n",
    "    return -1\n",
    "\n",
    "# return next state and reward given current state and action\n",
    "def get_next_state(state, action):\n",
    "    i, j=state//num_rows, state%num_rows\n",
    "    \n",
    "    if(i==0 and j==0):\n",
    "        next_state=[0, 0]\n",
    "        return next_state[0]*num_rows+next_state[1]\n",
    "    \n",
    "    if(i==num_rows-1 and j==num_cols-1):\n",
    "        next_state=[num_rows-1, num_cols-1]\n",
    "        return next_state[0]*num_rows+next_state[1]\n",
    "    \n",
    "    if(i+action[0]>=0 and i+action[0]<=3 and j+action[1]>=0 and j+action[1]<=3):\n",
    "        next_state=[i+action[0], j+action[1]]\n",
    "        return next_state[0]*num_rows+next_state[1]\n",
    "    \n",
    "    next_state=[i, j]\n",
    "    return next_state[0]*num_rows+next_state[1]\n",
    "    \n",
    "\n",
    "# value iteration steps\n",
    "    \n",
    "# Policy evaluation\n",
    "while(True):\n",
    "    delta=0\n",
    "    for state in range(num_states):\n",
    "        v=state_value_function[state]\n",
    "        new_state_value=-sys.maxsize-1\n",
    "\n",
    "        # terminal states\n",
    "        if(state==0 or state==num_states-1):\n",
    "            continue\n",
    "\n",
    "        # get max state value action considering all actions    \n",
    "        for action_index, action in enumerate(actions):\n",
    "            new_state_value=max(get_reward(state, action)+state_value_function[get_next_state(state, actions[action])], new_state_value)\n",
    "\n",
    "        delta=max(delta, abs(v-new_state_value))\n",
    "        state_value_function[state]=new_state_value\n",
    "    if(delta<theta):\n",
    "        break\n",
    "    print('delta: ', delta)\n",
    "\n",
    "    print('Policy evaluation update:')\n",
    "    print(state_value_function.round(2))\n",
    "        \n",
    "# Policy improvement\n",
    "for state in range(num_states):\n",
    "\n",
    "    # terminal states\n",
    "    if(state==0 or state==num_states-1):\n",
    "        continue\n",
    "\n",
    "    old_action=deepcopy(pi_matrix[state])\n",
    "\n",
    "    max_action_value=-sys.maxsize-1\n",
    "\n",
    "    # get optimal actions' state value function\n",
    "    for action_index, action in enumerate(actions):\n",
    "        current_action_value=get_reward(state, action)+state_value_function[get_next_state(state, actions[action])]\n",
    "\n",
    "        if(current_action_value>max_action_value):\n",
    "            max_action_value=current_action_value\n",
    "\n",
    "            \n",
    "    # get optimal actions for each state stochastically \n",
    "    best_actions=[]\n",
    "\n",
    "    for action_index, action in enumerate(actions):\n",
    "        current_action_value=get_reward(state, action)+state_value_function[get_next_state(state, actions[action])]\n",
    "        if(current_action_value==max_action_value):\n",
    "            best_actions.append(action_index)\n",
    "    all_actions=[0, 1, 2, 3]\n",
    "    for other_action in all_actions:\n",
    "        if(other_action in best_actions):\n",
    "            pi_matrix[state][other_action]=1/len(best_actions)\n",
    "        else:\n",
    "            pi_matrix[state][other_action]=0\n",
    "\n",
    "print('Policy improvement update:')\n",
    "print(pi_matrix)\n",
    "\n",
    "print()\n",
    "print('Final optimal policy value function:')\n",
    "print(state_value_function.reshape((num_rows, num_cols)))\n",
    "print()\n",
    "print('Final optimal policy')\n",
    "\n",
    "# get optimal policy from mapping from state value function\n",
    "\n",
    "for s_index, state_policy in enumerate(pi_matrix):\n",
    "    if(s_index==0):\n",
    "        print('-,', end='')\n",
    "        continue\n",
    "    if(s_index==num_states-1):\n",
    "        print('-', end='')\n",
    "        continue\n",
    "        \n",
    "    max_s=max(state_policy)\n",
    "    \n",
    "    for val in range(len(state_policy)):\n",
    "        if(state_policy[val]==max_s):\n",
    "            print(mapping[val], end='')\n",
    "            \n",
    "    print(',', end='')\n",
    "\n",
    "    if(s_index!=0 and (s_index+1)%num_rows==0):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Jack's Car Rental part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacks' Car rental, eg. 4.2 reconstruction\n",
    "\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initialise parameters of MDP\n",
    "num_cars_A=21\n",
    "num_cars_B=21\n",
    "num_states=num_cars_A*num_cars_B\n",
    "grid_world=np.zeros(num_states)\n",
    "num_actions=11\n",
    "theta=1\n",
    "\n",
    "# expectations of poisson distributions for the requests and returns for each location \n",
    "expected_requests_A=3\n",
    "expected_requests_B=4\n",
    "expected_returns_A=3\n",
    "expected_returns_B=2\n",
    "\n",
    "# max number of rentals or returns in a single day possible\n",
    "max_number_cars_rented_or_returned=11\n",
    "# discount rate\n",
    "gamma=0.9\n",
    "\n",
    "# rewards as specified\n",
    "rent_reward=10\n",
    "moving_reward=-2\n",
    "\n",
    "# define the poisson distribution value for a given lambda and n\n",
    "def poisson(n, lambda_val):\n",
    "    return np.exp(-1*lambda_val)*np.power(lambda_val, n)/np.math.factorial(n)\n",
    "\n",
    "# method to return the expected return from a particular state given a chosen action\n",
    "def get_state_value(car_a, car_b, pi_matrix, state_value_function):\n",
    "    action=pi_matrix[car_a][car_b]\n",
    "    moved_cars_a=car_a-action\n",
    "    expected_return=moving_reward*abs(pi_matrix[car_a][car_b])\n",
    "    moved_cars_b=car_b+action\n",
    "    cars_in_a=max(0, min(moved_cars_a, num_cars_A-1))\n",
    "    cars_in_b=max(0, min(moved_cars_b, num_cars_B-1))\n",
    "    \n",
    "    for request_sample_a in range(max_number_cars_rented_or_returned):\n",
    "        for request_sample_b in range(max_number_cars_rented_or_returned):\n",
    "            for return_sample_a in range(max_number_cars_rented_or_returned):\n",
    "                for return_sample_b in range(max_number_cars_rented_or_returned):\n",
    "                    final_prob=poisson(request_sample_a, expected_requests_A)\n",
    "                    final_prob*=poisson(request_sample_b, expected_requests_B)\n",
    "                    final_prob*=poisson(return_sample_a, expected_returns_A)\n",
    "                    final_prob*=poisson(return_sample_b, expected_returns_B)\n",
    "                    \n",
    "                    rentals_A=min(cars_in_a, request_sample_a)\n",
    "                    rentals_B=min(cars_in_b, request_sample_b)\n",
    "                    \n",
    "                    cars_in_a_end=int(min(cars_in_a+return_sample_a-rentals_A, num_cars_A-1))\n",
    "                    cars_in_b_end=int(min(cars_in_b+return_sample_b-rentals_B, num_cars_B-1))\n",
    "                    \n",
    "                    reward=rent_reward*(rentals_A+rentals_B)\n",
    "                    \n",
    "                    expected_return+=final_prob*(reward+gamma*state_value_function[cars_in_a_end][cars_in_b_end])\n",
    "\n",
    "    return expected_return\n",
    "\n",
    "# policy matrix containing optimal actions for each state\n",
    "pi_matrix=np.zeros((num_cars_A, num_cars_B))\n",
    "# state value function for each state\n",
    "state_value_function=np.zeros((num_cars_A, num_cars_B))\n",
    "\n",
    "possible_actions=np.zeros(num_actions)\n",
    "start_action=-5\n",
    "for possible_action in range(num_actions):\n",
    "    possible_actions[possible_action]=start_action\n",
    "    start_action+=1\n",
    "\n",
    "\n",
    "# Policy iteration    \n",
    "while(True):\n",
    "    \n",
    "    # policy evaluation\n",
    "    while(True):\n",
    "        delta=0\n",
    "        \n",
    "        for car_a in range(num_cars_A):\n",
    "            for car_b in range(num_cars_B):\n",
    "                updated_value=get_state_value(car_a, car_b, pi_matrix, state_value_function)\n",
    "                delta=max(abs(state_value_function[car_a][car_b]-updated_value), delta)\n",
    "                state_value_function[car_a][car_b]=updated_value\n",
    "        \n",
    "        if(delta<theta):\n",
    "            break\n",
    "        \n",
    "        print('delta: ', delta)\n",
    "            \n",
    "    # policy improvement\n",
    "    policy_stable=True\n",
    "    for car_a in range(num_cars_A):\n",
    "        for car_b in range(num_cars_B):\n",
    "            \n",
    "            all_action_values=[]\n",
    "            \n",
    "            for action in possible_actions:\n",
    "                current_val=get_state_value(car_a, car_b, pi_matrix, state_value_function)\n",
    "                all_action_values.append(current_val)\n",
    "                \n",
    "            best_action=np.argmax(np.asarray(all_action_values))\n",
    "            if(best_action==pi_matrix[car_a][car_b]):\n",
    "                pass\n",
    "            else:\n",
    "                policy_stable=False\n",
    "                pi_matrix[car_a][car_b]=best_action\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(pi_matrix)\n",
    "    plt.pcolor(pi_matrix)\n",
    "    plt.show()\n",
    "    if(policy_stable==True):\n",
    "        break\n",
    "                \n",
    "\n",
    "plt.pcolor(pi_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(pi_matrix)\n",
    "plt.show()\n",
    "\n",
    "#Reference from: https://stackoverflow.com/questions/11766536/matplotlib-3d-surface-from-a-rectangular-array-of-heights\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X=np.arange(0,21)\n",
    "Y=np.arange(0,21)\n",
    "X,Y=np.meshgrid(X,Y)\n",
    "fig=plt.figure()\n",
    "ax=fig.gca(projection='3d')\n",
    "surf=ax.plot_surface(X,Y,state_value_function,rstride=1,cstride=1,cmap='hot',linewidth=0,antialiased=False)\n",
    "fig.colorbar(surf,shrink=0.5,aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Jack's Car Rental part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacks' Car rental, ex. 4.7\n",
    "\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initialise parameters of MDP\n",
    "num_cars_A=21\n",
    "num_cars_B=21\n",
    "num_states=num_cars_A*num_cars_B\n",
    "grid_world=np.zeros(num_states)\n",
    "num_actions=11\n",
    "theta=1\n",
    "\n",
    "# expectations of poisson distributions for the requests and returns for each location \n",
    "expected_requests_A=3\n",
    "expected_requests_B=4\n",
    "expected_returns_A=3\n",
    "expected_returns_B=2\n",
    "\n",
    "# max number of rentals or returns in a single day possible\n",
    "max_number_cars_rented_or_returned=11\n",
    "# discount rate\n",
    "gamma=0.9\n",
    "\n",
    "# rewards as specified\n",
    "rent_reward=10\n",
    "moving_reward=-2\n",
    "\n",
    "# poisson distribution value for a given lambda and n\n",
    "def poisson(n, lambda_val):\n",
    "    return np.exp(-1*lambda_val)*np.power(lambda_val, n)/np.math.factorial(n)\n",
    "\n",
    "# method to return expected return given a state and a chosen action\n",
    "def get_state_value(car_a, car_b, pi_matrix, state_value_function):\n",
    "    action=pi_matrix[car_a][car_b]\n",
    "    moved_cars_a=car_a-action\n",
    "    \n",
    "    # additional constraint specified in question 4.7:\n",
    "    # if we go from A to B, then number of cars transferred which is considered in reward becomes one less\n",
    "    # else if we go from B to A, number of cars transferred which is considered in reward is same\n",
    "    if(pi_matrix[car_a][car_b]>0):\n",
    "        expected_return=moving_reward*abs(pi_matrix[car_a][car_b]-1)\n",
    "    else:\n",
    "        expected_return=moving_reward*abs(pi_matrix[car_a][car_b])\n",
    "    \n",
    "    moved_cars_b=car_b+action\n",
    "    cars_in_a=max(0, min(moved_cars_a, num_cars_A-1))\n",
    "    cars_in_b=max(0, min(moved_cars_b, num_cars_B-1))\n",
    "    \n",
    "    for request_sample_a in range(max_number_cars_rented_or_returned):\n",
    "        for request_sample_b in range(max_number_cars_rented_or_returned):\n",
    "            for return_sample_a in range(max_number_cars_rented_or_returned):\n",
    "                for return_sample_b in range(max_number_cars_rented_or_returned):\n",
    "                    final_prob=poisson(request_sample_a, expected_requests_A)\n",
    "                    final_prob*=poisson(request_sample_b, expected_requests_B)\n",
    "                    final_prob*=poisson(return_sample_a, expected_returns_A)\n",
    "                    final_prob*=poisson(return_sample_b, expected_returns_B)\n",
    "                    \n",
    "                    rentals_A=min(cars_in_a, request_sample_a)\n",
    "                    rentals_B=min(cars_in_b, request_sample_b)\n",
    "                    \n",
    "                    cars_in_a_end=int(min(cars_in_a+return_sample_a-rentals_A, num_cars_A-1))\n",
    "                    cars_in_b_end=int(min(cars_in_b+return_sample_b-rentals_B, num_cars_B-1))\n",
    "                    \n",
    "                    reward=rent_reward*(rentals_A+rentals_B)\n",
    "                    \n",
    "                    # additional constraint mentioned in question:\n",
    "                    # if more than 10 cars present in A or B, additional cost of 10 incurred\n",
    "                    \n",
    "                    if(cars_in_a_end>10):\n",
    "                        reward-=4\n",
    "                        \n",
    "                    if(cars_in_b_end>10):\n",
    "                        reward-=4\n",
    "                    \n",
    "                    expected_return+=final_prob*(reward+gamma*state_value_function[cars_in_a_end][cars_in_b_end])\n",
    "\n",
    "    return expected_return\n",
    "\n",
    "\n",
    "# policy matrix containing optimal actions for each state\n",
    "pi_matrix=np.zeros((num_cars_A, num_cars_B))\n",
    "# state value function for each state\n",
    "state_value_function=np.zeros((num_cars_A, num_cars_B))\n",
    "\n",
    "possible_actions=np.zeros(num_actions)\n",
    "start_action=-5\n",
    "for possible_action in range(num_actions):\n",
    "    possible_actions[possible_action]=start_action\n",
    "    start_action+=1\n",
    "\n",
    "\n",
    "# Policy iteration    \n",
    "while(True):\n",
    "    \n",
    "    # policy evaluation\n",
    "    while(True):\n",
    "        delta=0\n",
    "        \n",
    "        for car_a in range(num_cars_A):\n",
    "            for car_b in range(num_cars_B):\n",
    "                updated_value=get_state_value(car_a, car_b, pi_matrix, state_value_function)\n",
    "                delta=max(abs(state_value_function[car_a][car_b]-updated_value), delta)\n",
    "                state_value_function[car_a][car_b]=updated_value\n",
    "        \n",
    "        if(delta<theta):\n",
    "            break\n",
    "        \n",
    "        print('delta: ', delta)\n",
    "#         print(state_value_function)\n",
    "            \n",
    "    # policy improvement\n",
    "    policy_stable=True\n",
    "    for car_a in range(num_cars_A):\n",
    "        for car_b in range(num_cars_B):\n",
    "            \n",
    "            all_action_values=[]\n",
    "            \n",
    "            for action in possible_actions:\n",
    "                current_val=get_state_value(car_a, car_b, pi_matrix, state_value_function)\n",
    "                all_action_values.append(current_val)\n",
    "                \n",
    "            best_action=np.argmax(np.asarray(all_action_values))\n",
    "            if(best_action==pi_matrix[car_a][car_b]):\n",
    "                pass\n",
    "            else:\n",
    "                policy_stable=False\n",
    "                pi_matrix[car_a][car_b]=best_action\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(pi_matrix)\n",
    "    plt.pcolor(pi_matrix)\n",
    "    plt.show()\n",
    "    if(policy_stable==True):\n",
    "        break\n",
    "                \n",
    "\n",
    "plt.pcolor(pi_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(pi_matrix)\n",
    "plt.show()\n",
    "\n",
    "#Reference from: https://stackoverflow.com/questions/11766536/matplotlib-3d-surface-from-a-rectangular-array-of-heights\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X=np.arange(0,21)\n",
    "Y=np.arange(0,21)\n",
    "X,Y=np.meshgrid(X,Y)\n",
    "fig=plt.figure()\n",
    "ax=fig.gca(projection='3d')\n",
    "surf=ax.plot_surface(X,Y,state_value_function,rstride=1,cstride=1,cmap='hot',linewidth=0,antialiased=False)\n",
    "fig.colorbar(surf,shrink=0.5,aspect=5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
