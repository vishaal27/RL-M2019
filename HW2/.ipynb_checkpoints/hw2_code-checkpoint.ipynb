{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: GridWorld with solving Bellman equations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.3,  8.8,  4.4,  5.3,  1.5],\n",
       "       [ 1.5,  3. ,  2.3,  1.9,  0.5],\n",
       "       [ 0.1,  0.7,  0.7,  0.4, -0.4],\n",
       "       [-1. , -0.4, -0.4, -0.6, -1.2],\n",
       "       [-1.9, -1.3, -1.2, -1.4, -2. ]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# discount rate gamma\n",
    "discount_rate=0.9\n",
    "\n",
    "# constant variables regarding state environments\n",
    "num_rows=5\n",
    "num_cols=5\n",
    "num_states=25\n",
    "\n",
    "# coefficient matrix and constant vector for solving system of linear equations\n",
    "grid_world_state_coefficients=np.zeros((num_states, num_states))\n",
    "grid_world_constants=np.zeros(num_states)\n",
    "\n",
    "# possible action sequences in any given state\n",
    "actions={'north':(-1, 0), 'south':(1, 0), 'east':(0, 1), 'west':(0, -1)}\n",
    "\n",
    "# policy for each action action taken (in this case equiprobable)\n",
    "pi=0.25\n",
    "\n",
    "# Given a current state and an action taken, returns the next state and the reward obtained\n",
    "def next_state_and_reward(current_state, current_action):\n",
    "    reward=0\n",
    "    next_state=[0, 0]\n",
    "        \n",
    "    # if current state is state A, then reward is 10 and next state is A' regardless of the action taken\n",
    "    if(current_state[0]==0 and current_state[1]==1):\n",
    "        reward=10\n",
    "        next_state=[4, 1]\n",
    "        return reward, next_state\n",
    "    \n",
    "    # if current state is state B, then reward is 5 and next state is B' regardless of the action taken\n",
    "    elif(current_state[0]==0 and current_state[1]==3):\n",
    "        reward=5\n",
    "        next_state=[2, 3]\n",
    "        return reward, next_state\n",
    "    \n",
    "    # if next state that is reached based on action taken is within the grid world, then return next state with reward 0\n",
    "    if((current_state[0]+current_action[0])>=0 and (current_state[0]+current_action[0])<=4 and (current_state[1]+current_action[1])>=0 and (current_state[1]+current_action[1])<=4):\n",
    "        reward=0\n",
    "        next_state=[current_state[0]+current_action[0], current_state[1]+current_action[1]]        \n",
    "        return reward, next_state\n",
    "    \n",
    "    # if next state that is reached based on action taken is outside gridworld, then return next state as current state with reward -1\n",
    "    else:\n",
    "        reward=-1\n",
    "        next_state=[current_state[0], current_state[1]]\n",
    "        return reward, next_state\n",
    "\n",
    "# create coefficient and constant matrices for solving linear system of equations\n",
    "for state in range(num_states):\n",
    "    # The coefficient for a state s in its own state equation (row s) will be: \n",
    "    # c=1-n_r*(pi*prob(s,r|s,a)*gamma)\n",
    "    #\n",
    "    # The coefficient for a different state s' in a different state's state equation (row s) will be:\n",
    "    # c'=1-pi*prob(s',r|s,a)*gamma\n",
    "    #\n",
    "    # The constant term for an equation for state s (row s) will be:\n",
    "    # const=pi*(r1+r2+r3+r4)\n",
    "    # where r1 is the reward from action north, r2 is from action south, r3 from action east and r4 from action west\n",
    "    grid_world_state_coefficients[state][state]+=1\n",
    "    \n",
    "    for action in actions:\n",
    "        reward, next_state=next_state_and_reward([state%num_rows, state//num_rows], actions[action])\n",
    "        grid_world_state_coefficients[state, next_state[0]+next_state[1]*num_rows]-=pi*discount_rate\n",
    "        grid_world_constants[state]+=pi*reward\n",
    "    \n",
    "# Solve system of linear equations to get the values of each of the state-action value functions\n",
    "value_function=np.linalg.solve(grid_world_state_coefficients, grid_world_constants)\n",
    "value_function.round(1).reshape((num_rows, num_cols)).transpose()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Optimal State-Value Function and Optimal Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value function is:\n",
      "[[22.  24.4 22.  19.4 17.5]\n",
      " [19.8 22.  19.8 17.8 16. ]\n",
      " [17.8 19.8 17.8 16.  14.4]\n",
      " [16.  17.8 16.  14.4 13. ]\n",
      " [14.4 16.  14.4 13.  11.7]]\n",
      "\n",
      "The optimal policy is:\n",
      "E,SENW,W,SENW,W\n",
      "EN,N,NW,W,W\n",
      "EN,N,NW,NW,NW\n",
      "EN,N,NW,NW,NW\n",
      "EN,N,NW,NW,NW\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "# discount rate gamma\n",
    "discount_rate=0.9\n",
    "\n",
    "# constant variables regarding state environments\n",
    "num_rows=5\n",
    "num_cols=5\n",
    "num_states=25\n",
    "num_actions=4\n",
    "\n",
    "# coefficient matrix and constant vector for solving non linear system of equations for optimal action-value function\n",
    "optimal_action_coefficients=np.zeros((num_actions*num_states, num_states))\n",
    "optimal_action_constants=np.zeros(num_actions*num_states)\n",
    "\n",
    "# possible action sequences in any given state\n",
    "actions={'north':(-1, 0), 'south':(1, 0), 'east':(0, 1), 'west':(0, -1)}\n",
    "mapping={0:'S', 1:'E', 2:'N', 3:'W'}\n",
    "\n",
    "# policy for each action action taken (in this case equiprobable)\n",
    "pi=0.25\n",
    "\n",
    "# Given a current state and an action taken, returns the next state and the reward obtained\n",
    "def next_state_and_reward(current_state, current_action):\n",
    "    reward=0\n",
    "    next_state=[0, 0]\n",
    "        \n",
    "    # if current state is state A, then reward is 10 and next state is A' regardless of the action taken\n",
    "    if(current_state[0]==0 and current_state[1]==1):\n",
    "        reward=10\n",
    "        next_state=[4, 1]\n",
    "        return reward, next_state\n",
    "    \n",
    "    # if current state is state B, then reward is 5 and next state is B' regardless of the action taken\n",
    "    elif(current_state[0]==0 and current_state[1]==3):\n",
    "        reward=5\n",
    "        next_state=[2, 3]\n",
    "        return reward, next_state\n",
    "    \n",
    "    # if next state that is reached based on action taken is within the grid world, then return next state with reward 0\n",
    "    if((current_state[0]+current_action[0])>=0 and (current_state[0]+current_action[0])<=4 and (current_state[1]+current_action[1])>=0 and (current_state[1]+current_action[1])<=4):\n",
    "        reward=0\n",
    "        next_state=[current_state[0]+current_action[0], current_state[1]+current_action[1]]        \n",
    "        return reward, next_state\n",
    "    \n",
    "    # if next state that is reached based on action taken is outside gridworld, then return next state as current state with reward -1\n",
    "    else:\n",
    "        reward=-1\n",
    "        next_state=[current_state[0], current_state[1]]\n",
    "        return reward, next_state\n",
    "\n",
    "# create coefficient and constant matrices for solving non linear system of equations\n",
    "for state in range(num_states):\n",
    "    # The coefficient for a state s in its own state equation (row s) will be: \n",
    "    # c=1-n_r*(pi*prob(s,r|s,a)*gamma)\n",
    "    #\n",
    "    # The coefficient for a different state s' in a different state's state equation (row s) will be:\n",
    "    # c'=1-pi*prob(s',r|s,a)*gamma\n",
    "    #\n",
    "    # The constant term for an equation for state s (row s) will be:\n",
    "    # const=pi*(r1+r2+r3+r4)\n",
    "    # where r1 is the reward from action north, r2 is from action south, r3 from action east and r4 from action west\n",
    "    grid_world_state_coefficients[state][state]+=1\n",
    "    \n",
    "    for action_index, action in enumerate(actions):\n",
    "        reward, next_state=next_state_and_reward([state%num_rows, state//num_rows], actions[action])\n",
    "        optimal_action_coefficients[num_actions*state+action_index, state]+=1\n",
    "        optimal_action_coefficients[num_actions*state+action_index, next_state[0]+next_state[1]*num_rows]-=discount_rate\n",
    "        optimal_action_constants[num_actions*state+action_index]+=reward\n",
    "\n",
    "        \n",
    "# Solve system of linear equations to get the values of each of the state-action value functions\n",
    "optimal_value_function=optimize.linprog(np.ones(num_states), -optimal_action_coefficients, -optimal_action_constants)\n",
    "optimal_value_function=np.asarray(optimal_value_function.x).round(1)\n",
    "\n",
    "print('The optimal value function is:')\n",
    "print(np.asarray(optimal_value_function.reshape((num_rows, num_cols)).transpose()))\n",
    "\n",
    "print()\n",
    "print('The optimal policy is:')\n",
    "\n",
    "# To find optimal policy from the optimal value function\n",
    "for state in range(num_states):\n",
    "    \n",
    "    # For each state, find next state from all actions\n",
    "    q_pi_values=np.zeros(num_actions)\n",
    "    for action_index, action in enumerate(actions):\n",
    "        reward, next_state=next_state_and_reward([state//num_rows, state%num_rows], actions[action])\n",
    "        q_pi_values[action_index]=optimal_value_function[next_state[0]+next_state[1]*num_rows]\n",
    "    \n",
    "    max_action=np.max(q_pi_values)\n",
    "    \n",
    "    for q_pi_value_index, q_pi_value in enumerate(q_pi_values):\n",
    "        if(q_pi_value==max_action):\n",
    "            print(mapping[q_pi_value_index], end='')\n",
    "    if((state+1)%num_rows==0):\n",
    "        print()\n",
    "    else:\n",
    "        print(',', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Policy Iteration and Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      " 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "num_rows=4\n",
    "num_cols=4\n",
    "num_states=16\n",
    "grid_world=np.zeros(num_states)\n",
    "\n",
    "pi_matrix=np.zeros(num_states)\n",
    "pi_matrix.fill(1/num_rows)\n",
    "\n",
    "state_value_function=np.zeros(num_states)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
